{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8391517,"sourceType":"datasetVersion","datasetId":4991325}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom tqdm.auto import tqdm\nimport random\nimport torch\nfrom collections import defaultdict\nimport os\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nfrom transformers import BertTokenizer,BertModel","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-29T14:53:59.373386Z","iopub.execute_input":"2024-05-29T14:53:59.374224Z","iopub.status.idle":"2024-05-29T14:54:06.162998Z","shell.execute_reply.started":"2024-05-29T14:53:59.374188Z","shell.execute_reply":"2024-05-29T14:54:06.162174Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install lightning\n!pip install torchmetrics\n!pip install huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2024-05-29T13:49:43.984164Z","iopub.execute_input":"2024-05-29T13:49:43.984688Z","iopub.status.idle":"2024-05-29T13:50:26.174931Z","shell.execute_reply.started":"2024-05-29T13:49:43.984653Z","shell.execute_reply":"2024-05-29T13:50:26.173730Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting lightning\n  Downloading lightning-2.2.5-py3-none-any.whl.metadata (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.1)\nRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2024.2.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.11.2)\nRequirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.26.4)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\nRequirement already satisfied: torch<4.0,>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.1.2)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.3.2)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.9.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.2.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=1.13.0->lightning) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.13.0->lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.6)\nDownloading lightning-2.2.5-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: lightning\nSuccessfully installed lightning-2.2.5\nRequirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.3.2)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.1.2)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.11.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.0.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.22.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-05-29T14:35:29.046179Z","iopub.execute_input":"2024-05-29T14:35:29.046923Z","iopub.status.idle":"2024-05-29T14:35:29.067184Z","shell.execute_reply.started":"2024-05-29T14:35:29.046891Z","shell.execute_reply":"2024-05-29T14:35:29.066538Z"},"trusted":true},"execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16f314f1726746e59379489224747b85"}},"metadata":{}}]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\nseed_everything(42)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T14:54:06.164861Z","iopub.execute_input":"2024-05-29T14:54:06.165725Z","iopub.status.idle":"2024-05-29T14:54:06.173761Z","shell.execute_reply.started":"2024-05-29T14:54:06.165685Z","shell.execute_reply":"2024-05-29T14:54:06.172995Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"\ndata = pd.read_csv('/kaggle/input/aspect-data/aspect_based_data.csv')\nproducts = []\nfor idx,(product, group_data) in tqdm(enumerate(data.groupby(\"product\"))):\n    products.append(product)\nrandom.shuffle(products)\n\ntrain_ratio = 0.75\nval_ratio = 0.15\n\nsize = len(products)\n\ntrain = data.loc[data['product'].isin(products[:int(size*train_ratio)])]\nval = data.loc[data['product'].isin(products[int(size*train_ratio):int(size*(train_ratio+val_ratio))])]\ntest = data.loc[data['product'].isin(products[int(size*(train_ratio+val_ratio)):])]\n\nprint(f\"Number of train products: {len(train.groupby('product'))} --- Size: {len(train)}\")\nprint(f\"Number of validation products: {len(val.groupby('product'))} --- Size: {len(val)}\")\nprint(f\"Number of test products: {len(test.groupby('product'))} --- Size: {len(test)}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-29T13:50:26.218978Z","iopub.execute_input":"2024-05-29T13:50:26.219282Z","iopub.status.idle":"2024-05-29T13:50:26.317688Z","shell.execute_reply.started":"2024-05-29T13:50:26.219254Z","shell.execute_reply":"2024-05-29T13:50:26.316808Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04b29795960a47b7b1464dca91b680d0"}},"metadata":{}},{"name":"stdout","text":"Number of train products: 21 --- Size: 732\nNumber of validation products: 5 --- Size: 197\nNumber of test products: 3 --- Size: 71\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nclass ABSADataset(Dataset):\n    def __init__(self, df):\n        self.df = self.setup(df)\n        \n    \n    def setup(self,df):\n        new_df = []\n        for i,row in df.iterrows():\n            tokens, tags, pols = row[1:].values\n            tokens = tokens.replace(\"'\", \"\").strip(\"][\").split(', ')\n            tags = tags.strip('][').replace(\"'\", \"\").split(', ')\n            pols = pols.strip('][').replace(\"'\", \"\").split(', ')\n            tags = [int(i) for i in tags]\n            if sum(tags) != 0 and '0' not in pols:\n                new_df.append([tokens,tags,pols])\n        return new_df\n\n    def __getitem__(self, idx):\n        tokens, tags, pols = self.df[idx]\n\n      \n       \n        bert_att = []\n        pols_labels = []\n        start_ids = 0\n        end_ids = -1\n        for i in range(len(tokens)):\n            if int(tags[i]) == 1:\n                start_ids = i\n                end_ids = i\n            elif int(tags[i]) == 2:\n                end_ids += 1\n            elif int(tags[i]) == 0:\n                if start_ids <= end_ids:\n                    bert_att.append(tokens[start_ids:end_ids+1])\n                    pols_labels.append(int(pols[start_ids]) - 1 )\n                    end_ids = -1\n        if start_ids <= end_ids:\n            bert_att.append(tokens[start_ids:])\n            pols_labels.append(int(pols[start_ids]) - 1 ) \n       \n        id = np.random.randint(0,len(bert_att)) # in one sentence has one or more than one aspects\n        aspect = \" \".join(bert_att[id])\n        pols_label = pols_labels[id]\n       \n        \n        \n\n        return \" \".join(tokens),aspect,torch.tensor(pols_label)\n\n    def __len__(self):\n        return len(self.df)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T14:24:08.455854Z","iopub.execute_input":"2024-05-29T14:24:08.456367Z","iopub.status.idle":"2024-05-29T14:24:08.470754Z","shell.execute_reply.started":"2024-05-29T14:24:08.456335Z","shell.execute_reply":"2024-05-29T14:24:08.469689Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Build model","metadata":{}},{"cell_type":"code","source":"from transformers import BertModel\nfrom torch import nn\nimport torch\nfrom torch.utils.data import Dataset , DataLoader\nfrom huggingface_hub import PyTorchModelHubMixin\n\nclass ABSABert(nn.Module,PyTorchModelHubMixin):\n    def __init__(self,num_classes = 2):\n        super(ABSABert,self).__init__()\n        self.feature_extractor = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n        self.head = nn.Linear(768*2,num_classes)\n       \n    def forward(self, inputs1,inputs2):\n        outputs = self.feature_extractor(**inputs1)\n        cls_token1 = outputs.last_hidden_state[:,0]\n        \n        outputs2 = self.feature_extractor(**inputs2)\n        cls_token2 = outputs2.last_hidden_state[:,0]\n        \n        y = self.head(torch.cat([cls_token1,cls_token2],dim = -1)) # bs,d\n        return y","metadata":{"execution":{"iopub.status.busy":"2024-05-29T14:56:47.342570Z","iopub.execute_input":"2024-05-29T14:56:47.342945Z","iopub.status.idle":"2024-05-29T14:56:47.351362Z","shell.execute_reply.started":"2024-05-29T14:56:47.342917Z","shell.execute_reply":"2024-05-29T14:56:47.350257Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\ndef collate_fn(samples,tokenizer):\n    text = [s[0] for s in samples]\n    aspect = [s[1] for s in samples]\n    labels = torch.stack([s[2] for s in samples])\n    text_inputs = tokenizer(text,padding = True,truncation = True,max_length = 256,return_tensors=\"pt\")\n    aspect_inputs = tokenizer(aspect,padding = True,truncation = True,max_length = 256,return_tensors=\"pt\")\n    return text_inputs,aspect_inputs,labels\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T14:26:09.661780Z","iopub.execute_input":"2024-05-29T14:26:09.662653Z","iopub.status.idle":"2024-05-29T14:26:09.668929Z","shell.execute_reply.started":"2024-05-29T14:26:09.662621Z","shell.execute_reply":"2024-05-29T14:26:09.667833Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"import lightning.pytorch as L\nfrom torchmetrics.functional import accuracy, f1_score\nclass ABSABert_Module(L.LightningModule):\n    def __init__(self,train_data,val_data,test_data,lr,weight_decay,max_epochs,**kwargs):\n        self.save_hyperparameters()\n        super().__init__()\n        self.train_data = train_data\n        self.val_data = val_data\n        self.test_data = test_data\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.max_epochs = max_epochs\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n        self.loss_func = nn.CrossEntropyLoss()\n        self.model = ABSABert(num_classes = n_classes)\n        self.num_classes = n_classes\n    def setup(self, stage: str):\n        if stage == \"fit\":\n            self.train_dataset = ABSADataset(\n                self.train_data\n                )\n            self.val_dataset = ABSADataset(\n                self.val_data\n                )\n        if stage == \"test\":\n            self.test_dataset = ABSADataset(\n                self.test_data \n                )\n    def forward(self,inputs1,inputs2):\n        return self.model(inputs1,inputs2)\n\n    def training_step(self, batch, batch_idx):\n        inputs1,inputs2, label_ids = batch\n        y = label_ids\n        y_hat = self(inputs1,inputs2)\n        loss = self.loss_func(y_hat, label_ids)       \n        y_pred = torch.softmax(y_hat, dim=1)\n\n        # Logging to TensorBoard by default\n        self.log(\"train_loss\", loss, prog_bar=True,on_step=True, on_epoch=True)\n        self.log(\"train_acc\", accuracy(y_pred, y, task=\"multiclass\", num_classes=self.num_classes,ignore_index = 100), prog_bar=True,on_step=True, on_epoch=True)\n        self.log(\"train_f1\", f1_score(y_pred,y,average = 'macro', task=\"multiclass\", num_classes=self.num_classes,ignore_index=100), prog_bar=True,on_step=True, on_epoch=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        inputs1,inputs2, label_ids = batch\n      \n        y = label_ids\n        y_hat = self(inputs1,inputs2)\n        loss = self.loss_func(y_hat, label_ids)       \n        y_pred = torch.softmax(y_hat, dim=1)\n\n        y_pred = torch.softmax(y_hat, dim=1)\n       \n        \n        # Logging to TensorBoard by default\n        self.log(\"val_loss\", loss, prog_bar=True,on_step=False, on_epoch=True)\n        self.log(\"val_acc\", accuracy(y_pred, y, task=\"multiclass\", num_classes=self.num_classes,ignore_index=100), prog_bar=True,on_step=False, on_epoch=True)\n        self.log(\"val_f1\", f1_score(y_pred, y,average = 'macro', task=\"multiclass\", num_classes=self.num_classes,ignore_index=100), prog_bar=True,on_step=False, on_epoch=True)\n\n        return loss\n    def test_step(self, batch, batch_idx):\n        # OPTIONAL\n        inputs1,inputs2, label_ids = batch\n        y = label_ids\n        y_hat = self(inputs1,inputs2)\n        loss = self.loss_func(y_hat, label_ids)       \n        y_pred = torch.softmax(y_hat, dim=1)\n\n        self.log(\"test_acc\", accuracy(y_pred, y, task=\"multiclass\", num_classes=self.num_classes,ignore_index=100), prog_bar=True,on_step=False, on_epoch=True)\n        self.log(\"test_f1\", f1_score(y_pred, y,average = 'macro', task=\"multiclass\", num_classes=self.num_classes,ignore_index=100), prog_bar=True,on_step=False, on_epoch=True)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        if self.max_epochs is not None:\n            lr_scheduler = torch.optim.lr_scheduler.StepLR(\n                optimizer=optimizer,step_size = 10, gamma=0.7\n            )\n            return [optimizer], [lr_scheduler]\n        else:\n            return optimizer\n        \n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=16,shuffle = True,collate_fn=lambda batch: collate_fn(batch, self.tokenizer))\n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=16,collate_fn=lambda batch: collate_fn(batch, self.tokenizer))\n\n    def test_dataloader(self):\n        return DataLoader(self.test_dataset, batch_size=16,collate_fn=lambda batch: collate_fn(batch, self.tokenizer))\n        ","metadata":{"execution":{"iopub.status.busy":"2024-05-29T14:26:09.973204Z","iopub.execute_input":"2024-05-29T14:26:09.973821Z","iopub.status.idle":"2024-05-29T14:26:09.996198Z","shell.execute_reply.started":"2024-05-29T14:26:09.973793Z","shell.execute_reply":"2024-05-29T14:26:09.995275Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"import lightning.pytorch as L\nfrom lightning.pytorch.callbacks.early_stopping import EarlyStopping\nfrom  lightning.pytorch.callbacks import ModelCheckpoint\n\nlr = 1e-5\nweight_decay = 1e-4\nmax_epochs = 60\nn_classes = 2 #Positive, Negative\n\ntrainer = L.Trainer(\n    callbacks=[\n        EarlyStopping(monitor=\"val_loss\", mode=\"min\",min_delta=0.00, patience=15,check_finite = True ),\n        ModelCheckpoint(dirpath=\"/kaggle/working/\", save_top_k=1, monitor=\"val_loss\",save_last = True)\n              ],\n    max_epochs=max_epochs,\n    accelerator=\"auto\", devices='auto',\n    gradient_clip_val=1,\n    log_every_n_steps = 1\n)\nmodel = ABSABert_Module(train,val,test,lr,weight_decay,max_epochs)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T14:26:10.676437Z","iopub.execute_input":"2024-05-29T14:26:10.677405Z","iopub.status.idle":"2024-05-29T14:26:11.493437Z","shell.execute_reply.started":"2024-05-29T14:26:10.677369Z","shell.execute_reply":"2024-05-29T14:26:11.492574Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.fit(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T14:26:11.578473Z","iopub.execute_input":"2024-05-29T14:26:11.579038Z","iopub.status.idle":"2024-05-29T14:31:58.160811Z","shell.execute_reply.started":"2024-05-29T14:26:11.579008Z","shell.execute_reply":"2024-05-29T14:31:58.159943Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name      | Type             | Params\n-----------------------------------------------\n0 | loss_func | CrossEntropyLoss | 0     \n1 | model     | ABSABert         | 177 M \n-----------------------------------------------\n177 M     Trainable params\n0         Non-trainable params\n177 M     Total params\n711.426   Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47567733bcae4e4a9e1089d7ec82aa62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"code","source":"trainer.test(model,ckpt_path = '/kaggle/working/epoch=4-step=185.ckpt')","metadata":{"execution":{"iopub.status.busy":"2024-05-29T14:34:40.539277Z","iopub.execute_input":"2024-05-29T14:34:40.539931Z","iopub.status.idle":"2024-05-29T14:34:43.033025Z","shell.execute_reply.started":"2024-05-29T14:34:40.539901Z","shell.execute_reply":"2024-05-29T14:34:43.032348Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"INFO: Restoring states from the checkpoint path at /kaggle/working/epoch=4-step=185.ckpt\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: Loaded model weights from the checkpoint at /kaggle/working/epoch=4-step=185.ckpt\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b19813583822438986a5e2c5194cefc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.914893627166748    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8806527256965637    \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.914893627166748     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8806527256965637     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}},{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"[{'test_acc': 0.914893627166748, 'test_f1': 0.8806527256965637}]"},"metadata":{}}]},{"cell_type":"code","source":"model = ABSABert_Module.load_from_checkpoint(\"/kaggle/working/epoch=4-step=185.ckpt\")\n# save locally\nmodel.model.save_pretrained(\"Aspect_Based_Sentiment_Analysis_for_Reviews\")\n\n# push to the hub\nmodel.model.push_to_hub(\"Aspect_Based_Sentiment_Analysis_for_Reviews\")","metadata":{"execution":{"iopub.status.busy":"2024-05-29T14:35:51.623275Z","iopub.execute_input":"2024-05-29T14:35:51.624323Z","iopub.status.idle":"2024-05-29T14:36:20.791292Z","shell.execute_reply.started":"2024-05-29T14:35:51.624287Z","shell.execute_reply":"2024-05-29T14:36:20.790550Z"},"trusted":true},"execution_count":54,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/711M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afcada5143f64c69b1df76f771316536"}},"metadata":{}},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/NCTuanAnh/Aspect_Based_Sentiment_Analysis_for_Reviews/commit/6888255274fe2bdeb2022528e2e98f26e97d8f8e', commit_message='Push model using huggingface_hub.', commit_description='', oid='6888255274fe2bdeb2022528e2e98f26e97d8f8e', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"def predict(model,sentence,aspect ,device = 'cpu'):\n    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n    sentence = sentence.strip().lower()\n    aspect = aspect.strip().lower()\n\n    text_inputs = tokenizer(sentence,padding = True,truncation = True,max_length = 256,return_tensors=\"pt\")\n    aspect_inputs = tokenizer(aspect,padding = True,truncation = True,max_length = 256,return_tensors=\"pt\")\n    \n    output = model(text_inputs,aspect_inputs).argmax(dim = -1)\n    print(output)\n    \n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-29T15:17:53.579783Z","iopub.execute_input":"2024-05-29T15:17:53.580762Z","iopub.status.idle":"2024-05-29T15:17:53.588409Z","shell.execute_reply.started":"2024-05-29T15:17:53.580721Z","shell.execute_reply":"2024-05-29T15:17:53.587212Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"model = ABSABert.from_pretrained('NCTuanAnh/Aspect_Based_Sentiment_Analysis_for_Reviews')","metadata":{"execution":{"iopub.status.busy":"2024-05-29T15:07:37.390702Z","iopub.execute_input":"2024-05-29T15:07:37.391054Z","iopub.status.idle":"2024-05-29T15:07:38.053283Z","shell.execute_reply.started":"2024-05-29T15:07:37.391026Z","shell.execute_reply":"2024-05-29T15:07:38.052242Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"predict(model,'Sản phẩm này rất tốt','sản phẩm')\nprint(\"1\")","metadata":{"execution":{"iopub.status.busy":"2024-05-29T15:18:27.405469Z","iopub.execute_input":"2024-05-29T15:18:27.405849Z","iopub.status.idle":"2024-05-29T15:18:27.837969Z","shell.execute_reply.started":"2024-05-29T15:18:27.405818Z","shell.execute_reply":"2024-05-29T15:18:27.836991Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"tensor([0])\n1\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}